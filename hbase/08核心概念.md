## 1. 核心结构
RDBMS底层使用B树、B+树的存储结构，hbase使用LSM树(log-structured merge tree)。
- B+树
>树状数据结构，保持数据稳定有序，自底向上插入(与二叉树相反)，通过最大化每个内部节点的子节点数目来减少树的高度，不经常发生平衡操作，增加效率。  
>>在范围内可变子节点的数目，不像自平衡二叉树经常重新平衡，eg.2-3B树，可能有2或3个子节点/内部节点。节点通常表示为一组有序的元素和子指针。
- LSM树  
>算法对索引变更延迟+批量处理，使用基于内存和磁盘的组件（类似归并排序），对索引值查询可用其组件访问(除加锁期间)。减少磁盘磁臂开销(寻道+转动)，最适用索引插入比查询操作更常见的情况，eg.历史记录表+日志文件。

>**思想：划分不同等级的树。**  
>- eg.二级树，一份索引数据由2棵树（内存，可能是B树、AVL树等+磁盘，是B树）组成。数据先插入内存的树，超阈值，从左至右遍历，合并内存中叶子节点+磁盘中叶子节点，达到磁盘存储页大小，持久化到磁盘+更新父节点对叶子节点指针。
>- 磁盘的叶子节点（非叶子节点也被缓存到内存中）合并后，旧数据复制一份，与内存的数据一起顺序写到磁盘。
磁盘中树的非叶子节点也缓存到内存，先查内存的树，无，则查磁盘的树。磁盘树过大(数据量过大)，合并会变慢。  
>- 解决手段：建立层次。eg.内存的树为C0，磁盘的树为C1，C2，C3，...,Cn，合并顺序(C0，C1),(C1，C2)……。时间越长，flush越多，产生很多存储文件，而所有数据按key排序，不用重排。删除操作，存储删除标记，查找时跳过标记的，合并重写时被标记的才被丢弃。

>**区别：**
>>区别在于使用硬件的方式，特别是磁盘。  
**磁盘角度：** RDBMS通常都是寻道型（寻道速度每年大概提升5%），由B或B+树结构决定，log(N)。LSM-Tree是传输型（cpu、ram和磁盘空间每18~24个月翻番），在大规模下，传输比寻道高效。

>**优缺点：**
>>- 无太多更新，B+树工作得很好，以较繁重的优化保证较低的访问时间。越快越多地把数据放到随机位置，页面会越快变得碎片化。最终，数据传入速度可能超过优化进程重写现存文件的速度。改删以磁盘寻道速率级别进行，受限于最差的那个磁盘性能。  
>>- LSM-Tree以磁盘传输速率级别进行，使用日志文件和一个内存存储结构把随机写转换为顺序写，且读写操作独立，不产生竞争。可更好的扩展大规模数据，保证较一致的插入速率。

## 2.底层持久化
- 存储架构  
>- 两种基本文件类型：用于WAL（write-ahead log）和用于实际数据存储。  
>- **工作流程：** 客户端先连接zookeeper qurom(持有-ROOT-Region的服务器名，根据该信息访问拥有它的regionserver)，得到持有对应行键的.META.表region的服务器名。两个操作都会被缓存下来，最后查询.META.服务器，检索包含给定行键的region所在服务器。
>>启动hbase时，hmaster负责把region分配给每个hregionserver，包括-ROOT-和.META.表。
>- 结构类：hregionserver打开region，创建对应的hregion对象。当hregion被打开，就会为每个hcolumnfamily创建一个store实例(包含多个storefile实例(对hfile存储文件的简单封装)+一个memstore+一个由hregionserver共享的hlog实例(WAL相关类))。
